# -*- coding: utf-8 -*-
"""llama_sentiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KNrrRl6wj0pQEAYxdGaB_-qK5pyhsVaG
"""

!pip install bitsandbytes

!pip install huggingface_hub[hf_xet]

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline, TorchAoConfig
from huggingface_hub import login
import pandas as pd
import re

# Hugging Face token set up
# !pip install -q huggingface_hub
login(token="TOKEN")

class SentimentAnalyzer:
    def __init__(self, model_name="meta-llama/Meta-Llama-3-8B"):
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
        self._load_model_and_tokenizer()

    def _load_model_and_tokenizer(self):
        print(f"Loading model: {self.model_name} and tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, padding_side="left")

        # Configure BitsAndBytesConfig for quantization if needed for memory
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
        )

        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16,   # Required for GPU compatibility and performance
            quantization_config=bnb_config,
            device_map="auto" # Automatically maps layers to available devices (GPU first)
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        print("Model and tokenizer loaded.")


    def get_sentiment_llama3(self, texts):
        """
        Analyzes the sentiment of a batch of texts using the loaded Llama model.
        Args:
            texts (list): A list of text strings to analyze.
        Returns:
            list: A list of sentiment scores (integers from 1 to 10) for each text,
                  or None if a score could not be extracted.
        """

        sentiment_scores = []
        system_prompt = (
            '''You are an experienced financial analyst; analyze the text in { } carefully and assign a sentiment score to it. Please provide ONLY a single number as your answer.

            Scoring Guide:
            0: Negative sentiment
            1: Neutral sentiment
            2: Positive sentiment

            Example 1:
            {The company is bankrupt and sales are declining fast.}
            Answer: 0

            Example 2:
            {Earnings remained stable, and outlook is neutral pending market changes.}
            Answer: 1

            Example 3:
            {Revenue, profits, and guidance all significantly exceeded expectations.}
            Answer: 2'''
        )

        try:
            prompts = [f"<|system|>\n{system_prompt}\n<|user|>\n{{{text}}}\n<|assistant|>\n" for text in texts]

            inputs = self.tokenizer(prompts, return_tensors="pt", padding=True, truncation=True).to(self.model.device)

            outputs = self.model.generate(
                **inputs,
                max_new_tokens=5,
                do_sample=False
            )

            decoded_outputs = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)

            for i, (text, decoded) in enumerate(zip(texts, decoded_outputs)):
                # print(f"Decoded output: {decoded}") # print output for debugging
                match = re.search(r"<\|assistant\|>\s*(\d+)\b", decoded)
                if not match:
                    # Try fallback: find last integer in the decoded string
                    numbers = [int(s) for s in re.findall(r"\b(\d+)\b", decoded)]
                    score = numbers[-1] if numbers else None
                else:
                    score = int(match.group(1))
                sentiment_scores.append(score)


        except Exception as e:
            print(f"Error processing batch: {e}")
            sentiment_scores.extend([None] * len(texts))

            sentiment_scores.append(score)

        return sentiment_scores


    def process_dataframe(self, df, text_column, batch_size=10):
        """
        Processes a pandas DataFrame to add a sentiment score column.
        Args:
            df (pd.DataFrame): The input DataFrame.
            text_column (str): The name of the column containing the text to analyze.
            batch_size (int): The number of texts to process in each batch.
        Returns:
            pd.DataFrame: The DataFrame with a new 'sentiment_score' column.
        """
        all_sentiment_scores = []
        total_batches = (len(df) + batch_size - 1) // batch_size
        print(f"Processing {len(df)} texts in {total_batches} batches with batch size {batch_size}...")

        for i in range(0, len(df), batch_size):
            print(f"Processing batch {i // batch_size + 1}/{total_batches}...")
            batch_texts = df[text_column][i:i+batch_size].tolist()
            batch_sentiments = self.get_sentiment_llama3(batch_texts)
            all_sentiment_scores.extend(batch_sentiments)

        df['sentiment_score'] = all_sentiment_scores
        return df


data = {'component_text': [
    "We will lose money.",
    "Sales are booming!",
    "The market is stable.",
    "Profit margins are decreasing.",
    "New product launch expected to boost revenue.",
    "Uncertainty in global economy.",
    "Company acquired by a larger firm.",
    "Investment in R&D is increasing.",
    "Facing supply chain issues.",
    "Excellent quarterly results reported."
]}
df = pd.DataFrame(data)

# Process the DataFrame
analyzer = SentimentAnalyzer()
df_with_sentiment = analyzer.process_dataframe(df, 'component_text', batch_size=10) # Adjusted batch size for testing

# Display the updated DataFrame
print("\nDataFrame with Sentiment Scores:")
df_with_sentiment